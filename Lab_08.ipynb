{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharun173/CSE22173-ML-LAB/blob/main/Lab_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mh3Y_rZX5Sn",
        "outputId": "e540ed0e-968c-4a91-920c-566d3b711fdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Perceptron Params: {'penalty': 'l1', 'max_iter': 1000, 'alpha': 0.7742636826811278}\n",
            "Perceptron Test Score: 0.042222222222222223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best MLP Params: {'solver': 'adam', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50,), 'alpha': 10.0, 'activation': 'relu'}\n",
            "MLP Test Score: 0.7044444444444444\n",
            "Perceptron Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        3333       0.00      0.00      0.00         9\n",
            "        3334       1.00      0.71      0.83         7\n",
            "        3335       0.00      0.00      0.00        13\n",
            "        3337       0.00      0.00      0.00         5\n",
            "        3342       0.00      0.00      0.00        15\n",
            "        3343       0.00      0.00      0.00         1\n",
            "        3346       0.00      0.00      0.00         8\n",
            "        3349       0.00      0.00      0.00        21\n",
            "        3350       0.00      0.00      0.00         1\n",
            "        3351       0.00      0.00      0.00         7\n",
            "        3352       0.00      0.00      0.00        15\n",
            "        3353       0.00      0.00      0.00        13\n",
            "        3354       0.00      0.00      0.00        16\n",
            "        3356       1.00      0.17      0.29         6\n",
            "        3357       0.00      0.00      0.00         1\n",
            "        3358       0.00      0.00      0.00         1\n",
            "        3359       0.00      0.00      0.00        17\n",
            "        3360       0.00      0.00      0.00         7\n",
            "        3361       0.00      0.00      0.00         3\n",
            "        3362       0.00      0.00      0.00         3\n",
            "        3363       0.00      0.00      0.00        10\n",
            "        3364       0.00      0.00      0.00         5\n",
            "        3365       0.00      0.00      0.00         4\n",
            "        3366       0.00      0.00      0.00         8\n",
            "        3367       0.00      0.00      0.00         6\n",
            "        3368       0.00      0.00      0.00        18\n",
            "        3370       0.00      0.00      0.00         7\n",
            "        3371       0.00      0.00      0.00         3\n",
            "        3372       0.00      0.00      0.00         5\n",
            "        3373       0.00      0.00      0.00        12\n",
            "        3374       0.00      0.00      0.00        20\n",
            "        3375       0.00      0.00      0.00        14\n",
            "        3376       0.67      0.09      0.15        23\n",
            "        3377       0.00      0.00      0.00        15\n",
            "        3378       0.00      0.00      0.00         9\n",
            "        3379       0.00      0.00      0.00         7\n",
            "        3380       0.00      0.00      0.00         2\n",
            "        3381       0.00      0.00      0.00        14\n",
            "        3382       0.00      0.00      0.00        10\n",
            "        3383       0.00      0.00      0.00        14\n",
            "        3384       0.00      0.00      0.00        17\n",
            "        3385       0.00      0.00      0.00        17\n",
            "        3450       0.00      0.00      0.00         6\n",
            "        3451       0.03      1.00      0.05        11\n",
            "        3452       0.00      0.00      0.00        16\n",
            "        3453       0.00      0.00      0.00         5\n",
            "        3454       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.04       450\n",
            "   macro avg       0.06      0.04      0.03       450\n",
            "weighted avg       0.06      0.04      0.03       450\n",
            "\n",
            "MLP Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        3333       0.67      0.22      0.33         9\n",
            "        3334       1.00      1.00      1.00         7\n",
            "        3335       0.87      1.00      0.93        13\n",
            "        3337       0.57      0.80      0.67         5\n",
            "        3342       0.82      0.60      0.69        15\n",
            "        3343       0.00      0.00      0.00         1\n",
            "        3346       0.89      1.00      0.94         8\n",
            "        3349       0.93      0.62      0.74        21\n",
            "        3350       0.00      0.00      0.00         1\n",
            "        3351       0.00      0.00      0.00         7\n",
            "        3352       0.50      0.47      0.48        15\n",
            "        3353       0.81      1.00      0.90        13\n",
            "        3354       0.93      0.81      0.87        16\n",
            "        3355       0.00      0.00      0.00         0\n",
            "        3356       0.62      0.83      0.71         6\n",
            "        3357       0.00      0.00      0.00         1\n",
            "        3358       0.00      0.00      0.00         1\n",
            "        3359       0.92      0.65      0.76        17\n",
            "        3360       0.58      1.00      0.74         7\n",
            "        3361       1.00      0.67      0.80         3\n",
            "        3362       0.75      1.00      0.86         3\n",
            "        3363       1.00      0.80      0.89        10\n",
            "        3364       0.22      0.40      0.29         5\n",
            "        3365       0.33      0.50      0.40         4\n",
            "        3366       0.71      0.62      0.67         8\n",
            "        3367       0.83      0.83      0.83         6\n",
            "        3368       0.83      0.56      0.67        18\n",
            "        3370       0.75      0.43      0.55         7\n",
            "        3371       0.25      0.67      0.36         3\n",
            "        3372       0.62      1.00      0.77         5\n",
            "        3373       0.71      0.83      0.77        12\n",
            "        3374       0.76      0.65      0.70        20\n",
            "        3375       0.80      0.57      0.67        14\n",
            "        3376       0.68      0.57      0.62        23\n",
            "        3377       0.85      0.73      0.79        15\n",
            "        3378       0.41      0.78      0.54         9\n",
            "        3379       0.67      0.86      0.75         7\n",
            "        3380       0.50      1.00      0.67         2\n",
            "        3381       0.67      0.86      0.75        14\n",
            "        3382       0.50      0.70      0.58        10\n",
            "        3383       0.71      0.71      0.71        14\n",
            "        3384       0.74      0.82      0.78        17\n",
            "        3385       1.00      0.65      0.79        17\n",
            "        3450       1.00      1.00      1.00         6\n",
            "        3451       1.00      0.91      0.95        11\n",
            "        3452       0.61      0.69      0.65        16\n",
            "        3453       0.80      0.80      0.80         5\n",
            "        3454       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           0.70       450\n",
            "   macro avg       0.64      0.66      0.63       450\n",
            "weighted avg       0.75      0.70      0.71       450\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path, sheet_name=0):\n",
        "    \"\"\"Load the dataset from an csv file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Hyperparameter tuning for Perceptron\n",
        "def tune_perceptron(X_train, y_train):\n",
        "    \"\"\"Use RandomizedSearchCV to tune Perceptron hyperparameters.\"\"\"\n",
        "    perceptron = Perceptron()\n",
        "    param_dist = {\n",
        "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "        'alpha': np.logspace(-4, 1, 10),\n",
        "        'max_iter': [1000, 2000, 3000]\n",
        "    }\n",
        "    random_search = RandomizedSearchCV(perceptron, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search\n",
        "\n",
        "# Hyperparameter tuning for MLP\n",
        "def tune_mlp(X_train, y_train):\n",
        "    \"\"\"Use RandomizedSearchCV to tune MLPClassifier hyperparameters.\"\"\"\n",
        "    mlp = MLPClassifier()\n",
        "    param_dist = {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['tanh', 'relu'],\n",
        "        'solver': ['adam', 'sgd'],\n",
        "        'alpha': np.logspace(-4, 1, 10),\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    }\n",
        "    random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search\n",
        "\n",
        "# Main function for A2\n",
        "def run_a2_hyperparameter_tuning(file_path):\n",
        "    \"\"\"Run hyperparameter tuning for Perceptron and MLP models.\"\"\"\n",
        "    # Load data\n",
        "    data = load_data(file_path)\n",
        "    X = data.iloc[:, :-1].values  # Features\n",
        "    y = data.iloc[:, -1].values  # Target\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tune Perceptron\n",
        "    perceptron_search = tune_perceptron(X_train, y_train)\n",
        "    print(f\"Best Perceptron Params: {perceptron_search.best_params_}\")\n",
        "    print(f\"Perceptron Test Score: {perceptron_search.score(X_test, y_test)}\")\n",
        "\n",
        "    # Tune MLP\n",
        "    mlp_search = tune_mlp(X_train, y_train)\n",
        "    print(f\"Best MLP Params: {mlp_search.best_params_}\")\n",
        "    print(f\"MLP Test Score: {mlp_search.score(X_test, y_test)}\")\n",
        "\n",
        "    # Classification report for the best models\n",
        "    y_pred_perceptron = perceptron_search.best_estimator_.predict(X_test)\n",
        "    y_pred_mlp = mlp_search.best_estimator_.predict(X_test)\n",
        "    print(\"Perceptron Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_perceptron))\n",
        "    print(\"MLP Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_mlp))\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = '/content/DCT_withoutduplicate 2.csv'\n",
        "run_a2_hyperparameter_tuning(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex6F4XiiZ0Xf",
        "outputId": "6a0f13a4-b4a0-4045-d218-e10806bea1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== A3: Classifier Comparison ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier: SVM\n",
            "Accuracy: 0.08222222222222222\n",
            "F1 Score: 0.04594993501402417\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n",
            "Classifier: Decision Tree\n",
            "Accuracy: 0.6466666666666666\n",
            "F1 Score: 0.650744746844296\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n",
            "Classifier: Random Forest\n",
            "Accuracy: 0.8888888888888888\n",
            "F1 Score: 0.8887939181654759\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n",
            "Classifier: AdaBoost\n",
            "Accuracy: 0.06888888888888889\n",
            "F1 Score: 0.024513750645137507\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n",
            "Classifier: Naive Bayes\n",
            "Accuracy: 0.54\n",
            "F1 Score: 0.5598657350102723\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n",
            "Classifier: GradientBoosting\n",
            "Accuracy: 0.7911111111111111\n",
            "F1 Score: 0.7949222342457046\n",
            "ROC AUC: N/A - issue with probabilities\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset from an Excel file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Fit and evaluate a classifier\n",
        "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Fit the classifier and evaluate its performance.\"\"\"\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "    # Try to calculate ROC AUC if classifier supports predict_proba or decision_function\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        try:\n",
        "            y_prob = clf.predict_proba(X_test)\n",
        "            metrics['ROC AUC'] = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "        except ValueError:\n",
        "            metrics['ROC AUC'] = \"N/A - issue with probabilities\"\n",
        "    else:\n",
        "        metrics['ROC AUC'] = \"N/A - no predict_proba\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Compare different classifiers\n",
        "def compare_classifiers(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Compare multiple classifiers and tabulate their results.\"\"\"\n",
        "    classifiers = {\n",
        "        'SVM': SVC(probability=True),  # Ensure SVM has probability enabled\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'AdaBoost': AdaBoostClassifier(),\n",
        "        'Naive Bayes': GaussianNB(),\n",
        "        'GradientBoosting': GradientBoostingClassifier()\n",
        "    }\n",
        "\n",
        "    # Evaluate each classifier and store results\n",
        "    results = {}\n",
        "    for name, clf in classifiers.items():\n",
        "        results[name] = evaluate_classifier(clf, X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Print the results\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"Classifier: {name}\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Main function for A3 (Classifier Comparison)\n",
        "def run_a3_classifier_comparison(file_path):\n",
        "    \"\"\"Run multiple classifiers and compare their results.\"\"\"\n",
        "    # Load data\n",
        "    data = load_data(file_path)\n",
        "    X = data.iloc[:, :-1].values  # Features\n",
        "    y = data.iloc[:, -1].values  # Target\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Compare classifiers\n",
        "    compare_classifiers(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = '/content/DCT_withoutduplicate 2.csv'\n",
        "\n",
        "# Run A3 - Classifier Comparison\n",
        "print(\"=== A3: Classifier Comparison ===\")\n",
        "run_a3_classifier_comparison(file_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRMblTKDaJkGamnFK/pzpF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}